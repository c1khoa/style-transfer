import onnxruntime as ort
import os
from typing import Optional

_sessions = {}

# Tá»± Ä‘á»™ng xÃ¡c Ä‘á»‹nh thÆ° má»¥c models dá»±a trÃªn vá»‹ trÃ­ file nÃ y
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))  # backend/
MODEL_DIR = os.path.join(BASE_DIR, "models")
ADAIN_MODEL_PATH = os.path.join(MODEL_DIR, "adain.onnx")
SANET_MODEL_PATH = os.path.join(MODEL_DIR, "sanet.onnx")

def load_model(model_name: str, providers: Optional[list] = None) -> ort.InferenceSession:
    """
    Load ONNX model vÃ o memory vÃ  tráº£ vá» InferenceSession.
    """
    print(f"ðŸŸ¢ Loading model: {model_name}...")
    
    if model_name not in ["adain", "sanet"]:
        raise ValueError(f"model_name pháº£i lÃ  'adain' hoáº·c 'sanet', nháº­n Ä‘Æ°á»£c: {model_name}")
    
    if model_name in _sessions:
        print(f"âš¡ Model '{model_name}' Ä‘Ã£ Ä‘Æ°á»£c load sáºµn, dÃ¹ng cache.")
        return _sessions[model_name]
    
    # Chá»n path model
    model_path = ADAIN_MODEL_PATH if model_name == "adain" else SANET_MODEL_PATH

    if not os.path.exists(model_path):
        raise FileNotFoundError(
            f"Model {model_name} khÃ´ng tÃ¬m tháº¥y táº¡i {model_path}. "
            f"HÃ£y cháº¡y convert_to_onnx.py Ä‘á»ƒ táº¡o file ONNX."
        )
    
    # Chá»n providers náº¿u chÆ°a cÃ³
    if providers is None:
        available_providers = ort.get_available_providers()
        providers = []

        if model_name == "sanet":
            if 'CUDAExecutionProvider' in available_providers:
                providers.append('CUDAExecutionProvider')
            elif 'ROCMExecutionProvider' in available_providers:
                providers.append('ROCMExecutionProvider')
            providers.append('CPUExecutionProvider')
        else:  # adain
            if 'DmlExecutionProvider' in available_providers:
                providers.append('DmlExecutionProvider')
            elif 'CUDAExecutionProvider' in available_providers:
                providers.append('CUDAExecutionProvider')
            elif 'ROCMExecutionProvider' in available_providers:
                providers.append('ROCMExecutionProvider')
            providers.append('CPUExecutionProvider')
    
    # Load ONNX Runtime session
    try:
        session = ort.InferenceSession(model_path, providers=providers)
        print(f"âœ… Model '{model_name}' loaded thÃ nh cÃ´ng vá»›i providers: {providers}")
    except Exception as e:
        if 'CPUExecutionProvider' not in providers:
            print(f"âš  GPU provider failed, fallback CPU: {e}")
            session = ort.InferenceSession(model_path, providers=['CPUExecutionProvider'])
        else:
            raise
    
    _sessions[model_name] = session
    return session

def get_model_info(model_name: str) -> dict:
    session = load_model(model_name)
    info = {
        "model_name": model_name,
        "inputs": [{"name": inp.name, "shape": inp.shape, "type": inp.type} for inp in session.get_inputs()],
        "outputs": [{"name": out.name, "shape": out.shape, "type": out.type} for out in session.get_outputs()]
    }
    return info

def clear_cache():
    """XÃ³a cache models."""
    global _sessions
    _sessions.clear()
    print("âš¡ Cache models Ä‘Ã£ Ä‘Æ°á»£c xÃ³a.")
