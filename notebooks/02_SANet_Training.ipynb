{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SANet Training\n",
        "\n",
        "Training SANet model trên Kaggle với T4 GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import os\n",
        "from PIL import Image, ImageFile\n",
        "import random\n",
        "import glob\n",
        "import numpy as np\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as F\n",
        "import pickle\n",
        "import shutil\n",
        "\n",
        "CONTENT_FOLDER = \"/kaggle/input/coco-2017-dataset/coco2017\"\n",
        "STYLE_FOLDER = \"/kaggle/input/wikisample/wikiart_sampled\"\n",
        "VGG_PATH = \"/kaggle/input/vggandatautils/vgg_normalised.pth\"\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "LR = 1e-4\n",
        "LR_DECAY = 5e-5\n",
        "MAX_ITER = 160000\n",
        "SAVE_INTERVAL = 5000\n",
        "LOG_INTERVAL = 1000\n",
        "PATIENCE = 999999\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calc_mean_std(feat, eps=1e-5):\n",
        "    size = feat.size()\n",
        "    assert (len(size) == 4)\n",
        "    N, C = size[:2]\n",
        "    feat_var = feat.view(N, C, -1).var(dim=2) + eps\n",
        "    feat_std = feat_var.sqrt().view(N, C, 1, 1)\n",
        "    feat_mean = feat.view(N, C, -1).mean(dim=2).view(N, C, 1, 1)\n",
        "    return feat_mean, feat_std\n",
        "\n",
        "def mean_variance_norm(feat):\n",
        "    size = feat.size()\n",
        "    mean, std = calc_mean_std(feat)\n",
        "    normalized_feat = (feat - mean.expand(size)) / std.expand(size)\n",
        "    return normalized_feat\n",
        "\n",
        "decoder = nn.Sequential(\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(512, 256, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(256, 256, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(256, 256, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(256, 256, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(256, 128, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(128, 128, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(128, 64, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(64, 64, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(64, 3, (3, 3)),\n",
        ")\n",
        "\n",
        "vgg = nn.Sequential(\n",
        "    nn.Conv2d(3, 3, (1, 1)),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(3, 64, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(64, 64, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d((2, 2), (2, 2), (0, 0), ceil_mode=True),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(64, 128, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(128, 128, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d((2, 2), (2, 2), (0, 0), ceil_mode=True),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(128, 256, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(256, 256, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(256, 256, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(256, 256, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d((2, 2), (2, 2), (0, 0), ceil_mode=True),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(256, 512, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(512, 512, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(512, 512, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(512, 512, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d((2, 2), (2, 2), (0, 0), ceil_mode=True),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(512, 512, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(512, 512, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(512, 512, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(512, 512, (3, 3)),\n",
        "    nn.ReLU()\n",
        ")\n",
        "\n",
        "class SANet(nn.Module):\n",
        "    def __init__(self, in_planes):\n",
        "        super(SANet, self).__init__()\n",
        "        self.f = nn.Conv2d(in_planes, in_planes, (1, 1))\n",
        "        self.g = nn.Conv2d(in_planes, in_planes, (1, 1))\n",
        "        self.h = nn.Conv2d(in_planes, in_planes, (1, 1))\n",
        "        self.sm = nn.Softmax(dim = -1)\n",
        "        self.out_conv = nn.Conv2d(in_planes, in_planes, (1, 1))\n",
        "        \n",
        "    def forward(self, content, style):\n",
        "        F = self.f(mean_variance_norm(content))\n",
        "        G = self.g(mean_variance_norm(style))\n",
        "        H = self.h(style)\n",
        "        b, c, h, w = F.size()\n",
        "        F = F.view(b, -1, w * h).permute(0, 2, 1)\n",
        "        b, c, h, w = G.size()\n",
        "        G = G.view(b, -1, w * h)\n",
        "        S = torch.bmm(F, G)\n",
        "        S = self.sm(S)\n",
        "        b, c, h, w = H.size()\n",
        "        H = H.view(b, -1, w * h)\n",
        "        O = torch.bmm(H, S.permute(0, 2, 1))\n",
        "        b, c, h, w = content.size()\n",
        "        O = O.view(b, c, h, w)\n",
        "        O = self.out_conv(O)\n",
        "        O += content\n",
        "        return O\n",
        "\n",
        "class Transform(nn.Module):\n",
        "    def __init__(self, in_planes):\n",
        "        super(Transform, self).__init__()\n",
        "        self.sanet4_1 = SANet(in_planes = in_planes)\n",
        "        self.sanet5_1 = SANet(in_planes = in_planes)\n",
        "        self.upsample5_1 = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        self.merge_conv_pad = nn.ReflectionPad2d((1, 1, 1, 1))\n",
        "        self.merge_conv = nn.Conv2d(in_planes, in_planes, (3, 3))\n",
        "        \n",
        "    def forward(self, content4_1, style4_1, content5_1, style5_1):\n",
        "        return self.merge_conv(self.merge_conv_pad(self.sanet4_1(content4_1, style4_1) + self.upsample5_1(self.sanet5_1(content5_1, style5_1))))\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Net, self).__init__()\n",
        "        enc_layers = list(encoder.children())\n",
        "        self.enc_1 = nn.Sequential(*enc_layers[:4])\n",
        "        self.enc_2 = nn.Sequential(*enc_layers[4:11])\n",
        "        self.enc_3 = nn.Sequential(*enc_layers[11:18])\n",
        "        self.enc_4 = nn.Sequential(*enc_layers[18:31])\n",
        "        self.enc_5 = nn.Sequential(*enc_layers[31:44])\n",
        "        self.transform = Transform(in_planes = 512)\n",
        "        self.decoder = decoder\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "        for name in ['enc_1', 'enc_2', 'enc_3', 'enc_4', 'enc_5']:\n",
        "            for param in getattr(self, name).parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def encode_with_intermediate(self, input):\n",
        "        results = [input]\n",
        "        for i in range(5):\n",
        "            func = getattr(self, 'enc_{:d}'.format(i + 1))\n",
        "            results.append(func(results[-1]))\n",
        "        return results[1:]\n",
        "\n",
        "    def calc_content_loss(self, input, target, norm = False):\n",
        "        if(norm == False):\n",
        "          return self.mse_loss(input, target)\n",
        "        else:\n",
        "          return self.mse_loss(mean_variance_norm(input), mean_variance_norm(target))\n",
        "\n",
        "    def calc_style_loss(self, input, target):\n",
        "        input_mean, input_std = calc_mean_std(input)\n",
        "        target_mean, target_std = calc_mean_std(target)\n",
        "        return self.mse_loss(input_mean, target_mean) + self.mse_loss(input_std, target_std)\n",
        "    \n",
        "    def forward(self, content, style):\n",
        "        style_feats = self.encode_with_intermediate(style)\n",
        "        content_feats = self.encode_with_intermediate(content)\n",
        "        stylized = self.transform(content_feats[3], style_feats[3], content_feats[4], style_feats[4])\n",
        "        g_t = self.decoder(stylized)\n",
        "        g_t_feats = self.encode_with_intermediate(g_t)\n",
        "        loss_c = self.calc_content_loss(g_t_feats[3], content_feats[3], norm = True) + self.calc_content_loss(g_t_feats[4], content_feats[4], norm = True)\n",
        "        loss_s = self.calc_style_loss(g_t_feats[0], style_feats[0])\n",
        "        for i in range(1, 5):\n",
        "            loss_s += self.calc_style_loss(g_t_feats[i], style_feats[i])\n",
        "        Icc = self.decoder(self.transform(content_feats[3], content_feats[3], content_feats[4], content_feats[4]))\n",
        "        Iss = self.decoder(self.transform(style_feats[3], style_feats[3], style_feats[4], style_feats[4]))\n",
        "        l_identity1 = self.calc_content_loss(Icc, content) + self.calc_content_loss(Iss, style)\n",
        "        Fcc = self.encode_with_intermediate(Icc)\n",
        "        Fss = self.encode_with_intermediate(Iss)\n",
        "        l_identity2 = self.calc_content_loss(Fcc[0], content_feats[0]) + self.calc_content_loss(Fss[0], style_feats[0])\n",
        "        for i in range(1, 5):\n",
        "            l_identity2 += self.calc_content_loss(Fcc[i], content_feats[i]) + self.calc_content_loss(Fss[i], style_feats[i])\n",
        "        return loss_c, loss_s, l_identity1, l_identity2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Utils\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransformImageNet:\n",
        "    def __init__(self, target_long=512, min_short=256, crop_size=None, \n",
        "                 gray_ratio=0.0, use_normalize=True):\n",
        "        self.target_long = target_long\n",
        "        self.min_short = min_short\n",
        "        self.crop_size = crop_size\n",
        "        self.gray_ratio = gray_ratio\n",
        "        self.use_normalize = use_normalize\n",
        "        self.to_tensor = T.ToTensor()\n",
        "        self.normalize = T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "\n",
        "    def resize_and_pad(self, img):\n",
        "        w, h = img.size\n",
        "        if w > h:\n",
        "            new_w = self.target_long\n",
        "            new_h = int(h * self.target_long / w)\n",
        "        else:\n",
        "            new_h = self.target_long\n",
        "            new_w = int(w * self.target_long / h)\n",
        "        img = img.resize((new_w, new_h), Image.Resampling.LANCZOS)\n",
        "        pad_w = max(0, self.min_short - new_w)\n",
        "        pad_h = max(0, self.min_short - new_h)\n",
        "        if pad_w > 0 or pad_h > 0:\n",
        "            img = F.pad(img, (0,0,pad_w,pad_h), fill=0)\n",
        "        return img\n",
        "\n",
        "    def __call__(self, img):\n",
        "        if random.random() < self.gray_ratio:\n",
        "            img = img.convert(\"L\").convert(\"RGB\")\n",
        "        img = self.resize_and_pad(img)\n",
        "        img = T.RandomHorizontalFlip(p=0.5)(img)\n",
        "        if self.crop_size:\n",
        "            img = T.RandomCrop(self.crop_size)(img)\n",
        "        img = self.to_tensor(img)\n",
        "        if self.use_normalize:\n",
        "            img = self.normalize(img)\n",
        "        return img\n",
        "\n",
        "def InfiniteSampler(n):\n",
        "    i = n - 1\n",
        "    order = np.random.permutation(n)\n",
        "    while True:\n",
        "        yield order[i]\n",
        "        i += 1\n",
        "        if i >= n:\n",
        "            np.random.seed()\n",
        "            order = np.random.permutation(n)\n",
        "            i = 0\n",
        "\n",
        "class InfiniteSamplerWrapper(torch.utils.data.sampler.Sampler):\n",
        "    def __init__(self, data_source):\n",
        "        self.num_samples = len(data_source)\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(InfiniteSampler(self.num_samples))\n",
        "\n",
        "    def __len__(self):\n",
        "        return 2 ** 31\n",
        "\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, content_folder, style_folder, \n",
        "                 content_subset, style_subset,\n",
        "                 transform=None, gray_ratio=0.0,\n",
        "                 valid_ext=('.jpg', '.jpeg', '.png', '.bmp', '.tiff')):\n",
        "        self.content_folder = os.path.join(content_folder, content_subset)\n",
        "        self.style_folder = os.path.join(style_folder, style_subset)\n",
        "\n",
        "        self.content_files = []\n",
        "        self.style_files = []\n",
        "\n",
        "        for ext in valid_ext:\n",
        "            self.content_files.extend(glob.glob(os.path.join(self.content_folder, f\"*{ext}\")))\n",
        "            self.style_files.extend(glob.glob(os.path.join(self.style_folder, f\"*{ext}\")))\n",
        "\n",
        "        self.content_files = sorted(self.content_files)\n",
        "        self.style_files = sorted(self.style_files)\n",
        "\n",
        "        if len(self.content_files) == 0:\n",
        "            raise RuntimeError(f\"No content images found in {self.content_folder}\")\n",
        "        if len(self.style_files) == 0:\n",
        "            raise RuntimeError(f\"No style images found in {self.style_folder}\")\n",
        "\n",
        "        self.transform = transform\n",
        "        self.gray_ratio = gray_ratio\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.content_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        content_path = self.content_files[idx]\n",
        "        content_img = Image.open(content_path).convert(\"RGB\")\n",
        "        \n",
        "        style_path = random.choice(self.style_files)\n",
        "        style_img = Image.open(style_path).convert(\"RGB\")\n",
        "        \n",
        "        if self.transform:\n",
        "            content_img = self.transform(content_img)\n",
        "            style_img = self.transform(style_img)\n",
        "\n",
        "        return content_img, style_img\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Extract Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "checkpoint_input = \"/kaggle/input/vggandresult\"\n",
        "\n",
        "if os.path.exists(f\"{checkpoint_input}/latest_checkpoint.pth\"):\n",
        "    print(\"Copying checkpoint files to /kaggle/working/...\")\n",
        "    \n",
        "    shutil.copy(f\"{checkpoint_input}/latest_checkpoint.pth\", \"/kaggle/working/\")\n",
        "    shutil.copy(f\"{checkpoint_input}/decoder_best.pth\", \"/kaggle/working/\")\n",
        "    shutil.copy(f\"{checkpoint_input}/transformer_best.pth\", \"/kaggle/working/\")\n",
        "    \n",
        "    if os.path.exists(f\"{checkpoint_input}/checkpoints\"):\n",
        "        shutil.copytree(f\"{checkpoint_input}/checkpoints\", \"/kaggle/working/checkpoints\")\n",
        "    \n",
        "    if os.path.exists(f\"{checkpoint_input}/samples\"):\n",
        "        shutil.copytree(f\"{checkpoint_input}/samples\", \"/kaggle/working/samples\")\n",
        "    \n",
        "    print(\"Checkpoint extracted! Training will resume automatically.\")\n",
        "    print(f\"Files copied:\")\n",
        "    print(f\"  - latest_checkpoint.pth\")\n",
        "    print(f\"  - decoder_best.pth\")\n",
        "    print(f\"  - transformer_best.pth\")\n",
        "    print(f\"  - checkpoints/ folder\")\n",
        "    print(f\"  - samples/ folder\")\n",
        "else:\n",
        "    print(\"No checkpoint found. Starting from scratch.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset và DataLoader Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.backends.cudnn.benchmark = True\n",
        "Image.MAX_IMAGE_PIXELS = None\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "train_transform = TransformImageNet(\n",
        "    target_long=512,\n",
        "    min_short=256,\n",
        "    crop_size=256,\n",
        "    gray_ratio=0.0,\n",
        "    use_normalize=False\n",
        ")\n",
        "\n",
        "val_transform = TransformImageNet(\n",
        "    target_long=512,\n",
        "    min_short=256,\n",
        "    crop_size=256,\n",
        "    gray_ratio=0.0,\n",
        "    use_normalize=False\n",
        ")\n",
        "\n",
        "train_dataset = CustomImageDataset(\n",
        "    CONTENT_FOLDER,\n",
        "    STYLE_FOLDER,\n",
        "    content_subset=\"train2017\",\n",
        "    style_subset=\"train\",\n",
        "    transform=train_transform,\n",
        "    gray_ratio=0.0\n",
        ")\n",
        "\n",
        "val_dataset = CustomImageDataset(\n",
        "    CONTENT_FOLDER,\n",
        "    STYLE_FOLDER,\n",
        "    content_subset=\"val2017\",\n",
        "    style_subset=\"valid\",\n",
        "    transform=val_transform,\n",
        "    gray_ratio=0.0\n",
        ")\n",
        "\n",
        "train_iter = iter(DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    sampler=InfiniteSamplerWrapper(train_dataset),\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        "))\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(f\"Train dataset: {len(train_dataset)} content images, {len(train_dataset.style_files)} style images\")\n",
        "print(f\"Validation dataset: {len(val_dataset)} content images, {len(val_dataset.style_files)} style images\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\")\n",
        "\n",
        "vgg.load_state_dict(torch.load(VGG_PATH))\n",
        "vgg = nn.Sequential(*list(vgg.children())[:44])\n",
        "\n",
        "network = Net(vgg, decoder)\n",
        "network.to(device)\n",
        "network.train()\n",
        "\n",
        "optimizer = torch.optim.Adam([\n",
        "    {'params': network.decoder.parameters()},\n",
        "    {'params': network.transform.parameters()}\n",
        "], lr=LR)\n",
        "\n",
        "scaler = GradScaler()\n",
        "\n",
        "print(\"Model initialized and moved to GPU\")\n",
        "print(\"Mixed Precision Training enabled\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference & Testing Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def style_transfer_sanet(network, content_path, style_path, device, output_size=None):\n",
        "    import torchvision.utils as vutils\n",
        "    \n",
        "    network.eval()\n",
        "    \n",
        "    content_img = Image.open(content_path).convert(\"RGB\")\n",
        "    style_img = Image.open(style_path).convert(\"RGB\")\n",
        "    \n",
        "    if output_size:\n",
        "        transform = T.Compose([\n",
        "            T.Resize(output_size),\n",
        "            T.ToTensor()\n",
        "        ])\n",
        "    else:\n",
        "        transform = T.ToTensor()\n",
        "    \n",
        "    content = transform(content_img).unsqueeze(0).to(device)\n",
        "    style = transform(style_img).unsqueeze(0).to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        style_feats = network.encode_with_intermediate(style)\n",
        "        content_feats = network.encode_with_intermediate(content)\n",
        "        stylized = network.transform(\n",
        "            content_feats[3], style_feats[3],\n",
        "            content_feats[4], style_feats[4]\n",
        "        )\n",
        "        output = network.decoder(stylized)\n",
        "    \n",
        "    return output.clamp(0, 1)\n",
        "\n",
        "def test_sanet(network, content_dir, style_dir, device, checkpoint_path, save_dir=\"/kaggle/working/test_results\", num_samples=10):\n",
        "    import torchvision.utils as vutils\n",
        "    \n",
        "    if not os.path.exists(checkpoint_path):\n",
        "        print(f\"Error: Checkpoint not found at {checkpoint_path}\")\n",
        "        print(\"Available checkpoints:\")\n",
        "        checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "        if os.path.exists(checkpoint_dir):\n",
        "            for f in os.listdir(checkpoint_dir):\n",
        "                if f.endswith('.pth'):\n",
        "                    print(f\"  - {os.path.join(checkpoint_dir, f)}\")\n",
        "        return\n",
        "    \n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    \n",
        "    print(f\"Loading checkpoint from {checkpoint_path}\")\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    network.decoder.load_state_dict(checkpoint['decoder'])\n",
        "    network.transform.load_state_dict(checkpoint['transform'])\n",
        "    network.to(device)\n",
        "    network.eval()\n",
        "    \n",
        "    content_paths = sorted(glob.glob(os.path.join(content_dir, \"*.jpg\")))[:num_samples]\n",
        "    style_paths = sorted(glob.glob(os.path.join(style_dir, \"*.jpg\")))\n",
        "    \n",
        "    print(f\"Testing {len(content_paths)} content images...\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for idx, content_path in enumerate(content_paths):\n",
        "            style_path = random.choice(style_paths)\n",
        "            \n",
        "            pair_dir = os.path.join(save_dir, f\"pair_{idx+1}\")\n",
        "            os.makedirs(pair_dir, exist_ok=True)\n",
        "            \n",
        "            output = style_transfer_sanet(network, content_path, style_path, device, output_size=512)\n",
        "            \n",
        "            content_img = Image.open(content_path).convert(\"RGB\")\n",
        "            style_img = Image.open(style_path).convert(\"RGB\")\n",
        "            \n",
        "            transform = T.Compose([T.Resize(512), T.ToTensor()])\n",
        "            content_tensor = transform(content_img).unsqueeze(0)\n",
        "            style_tensor = transform(style_img).unsqueeze(0)\n",
        "            \n",
        "            vutils.save_image(content_tensor, os.path.join(pair_dir, \"content.jpg\"))\n",
        "            vutils.save_image(style_tensor, os.path.join(pair_dir, \"style.jpg\"))\n",
        "            vutils.save_image(output, os.path.join(pair_dir, \"output.jpg\"))\n",
        "    \n",
        "    print(f\"Testing completed! Results saved in {save_dir}\")\n",
        "\n",
        "print(\"Inference functions ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Loop với Early Stopping\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def adjust_learning_rate(optimizer, iteration):\n",
        "    lr = LR / (1.0 + LR_DECAY * iteration)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "    return lr\n",
        "\n",
        "def save_checkpoint(iteration, val_loss, best_loss, patience_counter, is_best=False):\n",
        "    os.makedirs(\"/kaggle/working/checkpoints\", exist_ok=True)\n",
        "    \n",
        "    checkpoint = {\n",
        "        'iteration': iteration,\n",
        "        'decoder': network.decoder.state_dict(),\n",
        "        'transform': network.transform.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "        'scaler': scaler.state_dict(),\n",
        "        'val_loss': val_loss,\n",
        "        'best_loss': best_loss,\n",
        "        'patience_counter': patience_counter\n",
        "    }\n",
        "    \n",
        "    torch.save(checkpoint, f\"/kaggle/working/checkpoints/checkpoint_iter_{iteration}.pth\")\n",
        "    torch.save(checkpoint, \"/kaggle/working/latest_checkpoint.pth\")\n",
        "    \n",
        "    if is_best:\n",
        "        torch.save(network.decoder.state_dict(), \"/kaggle/working/decoder_best.pth\")\n",
        "        torch.save(network.transform.state_dict(), \"/kaggle/working/transformer_best.pth\")\n",
        "\n",
        "def load_checkpoint(checkpoint_path):\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        print(f\"Loading checkpoint from {checkpoint_path}\")\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "        network.decoder.load_state_dict(checkpoint['decoder'])\n",
        "        network.transform.load_state_dict(checkpoint['transform'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        if 'scaler' in checkpoint:\n",
        "            scaler.load_state_dict(checkpoint['scaler'])\n",
        "        start_iter = checkpoint['iteration']\n",
        "        best_loss = checkpoint['best_loss']\n",
        "        patience_counter = checkpoint['patience_counter']\n",
        "        print(f\"Resume from iteration {start_iter} | Best loss: {best_loss:.4f} | Patience: {patience_counter}/{PATIENCE}\")\n",
        "        return start_iter, best_loss, patience_counter\n",
        "    return 0, float('inf'), 0\n",
        "\n",
        "def validate(val_loader, network, device):\n",
        "    network.eval()\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for content_images, style_images in val_loader:\n",
        "            content_images = content_images.to(device)\n",
        "            style_images = style_images.to(device)\n",
        "            \n",
        "            loss_c, loss_s, l_identity1, l_identity2 = network(content_images, style_images)\n",
        "            loss_c = 1.0 * loss_c\n",
        "            loss_s = 3.0 * loss_s\n",
        "            loss = loss_c + loss_s + 50 * l_identity1 + l_identity2\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "            \n",
        "            if num_batches >= 50:\n",
        "                break\n",
        "    \n",
        "    network.train()\n",
        "    return total_loss / num_batches\n",
        "\n",
        "def generate_sample(network, val_loader, device, iteration):\n",
        "    network.eval()\n",
        "    os.makedirs(\"/kaggle/working/samples\", exist_ok=True)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        sample_content, sample_style = next(iter(val_loader))\n",
        "        sample_content = sample_content[:1].to(device)\n",
        "        sample_style = sample_style[:1].to(device)\n",
        "        \n",
        "        style_feats = network.encode_with_intermediate(sample_style)\n",
        "        content_feats = network.encode_with_intermediate(sample_content)\n",
        "        stylized = network.transform(\n",
        "            content_feats[3], style_feats[3],\n",
        "            content_feats[4], style_feats[4]\n",
        "        )\n",
        "        output = network.decoder(stylized).clamp(0, 1)\n",
        "        \n",
        "        import torchvision.utils as vutils\n",
        "        vutils.save_image(output, f\"/kaggle/working/samples/output_{iteration}.jpg\")\n",
        "        vutils.save_image(sample_content, f\"/kaggle/working/samples/content_{iteration}.jpg\")\n",
        "        vutils.save_image(sample_style, f\"/kaggle/working/samples/style_{iteration}.jpg\")\n",
        "    \n",
        "    network.train()\n",
        "\n",
        "start_iter, best_loss, patience_counter = load_checkpoint(\"/kaggle/working/latest_checkpoint.pth\")\n",
        "\n",
        "history = {\n",
        "    'iterations': [],\n",
        "    'train_loss': [],\n",
        "    'train_loss_c': [],\n",
        "    'train_loss_s': [],\n",
        "    'train_loss_id1': [],\n",
        "    'train_loss_id2': [],\n",
        "    'val_loss': [],\n",
        "    'lr': []\n",
        "}\n",
        "\n",
        "print(\"Starting training...\")\n",
        "print(f\"Max iterations: {MAX_ITER}\")\n",
        "print(f\"Save interval: {SAVE_INTERVAL}\")\n",
        "print(f\"Log interval: {LOG_INTERVAL}\")\n",
        "print(f\"Patience: {PATIENCE}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for i in range(start_iter, MAX_ITER):\n",
        "    current_lr = adjust_learning_rate(optimizer, i)\n",
        "    \n",
        "    content_images, style_images = next(train_iter)\n",
        "    content_images = content_images.to(device)\n",
        "    style_images = style_images.to(device)\n",
        "    \n",
        "    loss_c, loss_s, l_identity1, l_identity2 = network(content_images, style_images)\n",
        "    loss_c = 1.0 * loss_c\n",
        "    loss_s = 3.0 * loss_s\n",
        "    loss = loss_c + loss_s + 50 * l_identity1 + l_identity2\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if (i + 1) % LOG_INTERVAL == 0:\n",
        "        history['iterations'].append(i + 1)\n",
        "        history['train_loss'].append(loss.item())\n",
        "        history['train_loss_c'].append(loss_c.item())\n",
        "        history['train_loss_s'].append(loss_s.item())\n",
        "        history['train_loss_id1'].append(l_identity1.item())\n",
        "        history['train_loss_id2'].append(l_identity2.item())\n",
        "        history['lr'].append(current_lr)\n",
        "        \n",
        "        print(f\"Iter: {i+1}/{MAX_ITER} | Loss: {loss.item():.4f} | Content: {loss_c.item():.4f} | Style: {loss_s.item():.4f} | Id1: {l_identity1.item():.4f} | Id2: {l_identity2.item():.4f}\")\n",
        "    \n",
        "    if (i + 1) % 10 == 0:\n",
        "        torch.cuda.empty_cache()\n",
        "    \n",
        "    if (i + 1) % SAVE_INTERVAL == 0:\n",
        "        val_loss = validate(val_loader, network, device)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        is_best = val_loss < best_loss\n",
        "        \n",
        "        if is_best:\n",
        "            best_loss = val_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "        \n",
        "        save_checkpoint(i + 1, val_loss, best_loss, patience_counter, is_best)\n",
        "        generate_sample(network, val_loader, device, i + 1)\n",
        "        print(f\"Checkpoint saved at {i+1} | Val Loss: {val_loss:.4f} | Best: {is_best} | BestLoss: {best_loss:.4f} | Patience: {patience_counter}/{PATIENCE}\")\n",
        "        \n",
        "        if patience_counter >= PATIENCE:\n",
        "            print(f\"Early stopping triggered at iteration {i+1}\")\n",
        "            print(f\"Best validation loss: {best_loss:.4f}\")\n",
        "            break\n",
        "\n",
        "print(\"-\" * 50)\n",
        "print(\"Training completed!\")\n",
        "print(f\"Final best loss: {best_loss:.4f}\")\n",
        "\n",
        "with open('/kaggle/working/training_history.pkl', 'wb') as f:\n",
        "    pickle.dump(history, f)\n",
        "print(\"Training history saved to training_history.pkl\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
