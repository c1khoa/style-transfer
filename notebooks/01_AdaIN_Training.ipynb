{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a824e49f",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3467b361",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as F\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import shutil\n",
    "import random\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def style_sampling(base_path, dest_path, n_samples=100, split=(0.8, 0.1, 0.1)):\n",
    "    \"\"\"\n",
    "    L·∫•y m·∫´u ·∫£nh t·ª´ m·ªói style v√† chia ƒë·ªÅu train/valid/test.\n",
    "    ƒê·∫∑t t√™n file d·∫°ng style_001.jpg, style_002.jpg...\n",
    "    \n",
    "    Args:\n",
    "        base_path (str): th∆∞ m·ª•c ch·ª©a c√°c folder style\n",
    "        dest_path (str): th∆∞ m·ª•c l∆∞u k·∫øt qu·∫£\n",
    "        n_samples (int): s·ªë ·∫£nh l·∫•y m·∫´u cho m·ªói style\n",
    "        split (tuple): t·ªâ l·ªá chia train/valid/test\n",
    "    \"\"\"\n",
    "    random.seed(2025)\n",
    "    \n",
    "    subsets = [\"train\", \"valid\", \"test\"]\n",
    "    for subset in subsets:\n",
    "        os.makedirs(os.path.join(dest_path, subset), exist_ok=True)\n",
    "    \n",
    "    styles = [d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))]\n",
    "    total_copied = 0\n",
    "\n",
    "    for style in tqdm(styles, desc=\"Sampling styles\"):\n",
    "        style_path = os.path.join(base_path, style)\n",
    "        # L·ªçc file ·∫£nh\n",
    "        image_files = [f for f in os.listdir(style_path) \n",
    "                       if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff'))]\n",
    "        if not image_files:\n",
    "            continue\n",
    "\n",
    "        # L·∫•y m·∫´u ng·∫´u nhi√™n\n",
    "        sample_imgs = random.sample(image_files, min(len(image_files), n_samples))\n",
    "        n = len(sample_imgs)\n",
    "\n",
    "        # S·ªë l∆∞·ª£ng cho train/valid/test (l√†m tr√≤n xu·ªëng)\n",
    "        n_train = int(n * split[0])\n",
    "        n_valid = int(n * split[1])\n",
    "        n_test = n - n_train - n_valid\n",
    "\n",
    "        # Chia ·∫£nh\n",
    "        split_dict = {\n",
    "            \"train\": sample_imgs[:n_train],\n",
    "            \"valid\": sample_imgs[n_train:n_train+n_valid],\n",
    "            \"test\": sample_imgs[n_train+n_valid:]\n",
    "        }\n",
    "\n",
    "        # Copy ·∫£nh sang folder t∆∞∆°ng ·ª©ng v·ªõi t√™n style_index.jpg\n",
    "        for subset, imgs in split_dict.items():\n",
    "            dest_folder = os.path.join(dest_path, subset)\n",
    "            for idx, img_name in enumerate(imgs, start=1):\n",
    "                src = os.path.join(style_path, img_name)\n",
    "                dst_name = f\"{style}_{idx:03d}.jpg\"  # style_001.jpg, style_002.jpg...\n",
    "                dst = os.path.join(dest_folder, dst_name)\n",
    "                shutil.copy(src, dst)\n",
    "                total_copied += 1\n",
    "\n",
    "    print(f\"ƒê√£ sao ch√©p {total_copied} ·∫£nh t·ª´ {len(styles)} style v√†o {dest_path}/train, valid, test\")\n",
    "\n",
    "\n",
    "# ------------------- Transform -------------------\n",
    "class TransformImageNet:\n",
    "    def __init__(self, target_long=512, min_short=256, crop_size=None, gray_ratio=0.0):\n",
    "        \"\"\"\n",
    "        target_long: c·∫°nh l·ªõn c·ªßa ·∫£nh sau resize\n",
    "        min_short: n·∫øu c·∫°nh nh·ªè < min_short, s·∫Ω padding\n",
    "        crop_size: n·∫øu mu·ªën crop, None = kh√¥ng crop\n",
    "        gray_ratio: x√°c su·∫•t chuy·ªÉn ·∫£nh sang grayscale\n",
    "        \"\"\"\n",
    "        self.target_long = target_long\n",
    "        self.min_short = min_short\n",
    "        self.crop_size = crop_size\n",
    "        self.gray_ratio = gray_ratio\n",
    "        self.to_tensor = T.ToTensor()\n",
    "        self.normalize = T.Normalize(mean=[0.485,0.456,0.406],\n",
    "                                     std=[0.229,0.224,0.225])\n",
    "\n",
    "    def resize_and_pad(self, img):\n",
    "        w, h = img.size\n",
    "        # scale t·ªâ l·ªá theo c·∫°nh l·ªõn\n",
    "        if w > h:\n",
    "            new_w = self.target_long\n",
    "            new_h = int(h * self.target_long / w)\n",
    "        else:\n",
    "            new_h = self.target_long\n",
    "            new_w = int(w * self.target_long / h)\n",
    "        img = img.resize((new_w, new_h), Image.Resampling.LANCZOS)\n",
    "\n",
    "        # padding n·∫øu c·∫°nh nh·ªè < min_short\n",
    "        pad_w = max(0, self.min_short - new_w)\n",
    "        pad_h = max(0, self.min_short - new_h)\n",
    "        if pad_w > 0 or pad_h > 0:\n",
    "            img = F.pad(img, (0,0,pad_w,pad_h), fill=0)\n",
    "        return img\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # Grayscale augmentation\n",
    "        if random.random() < self.gray_ratio:\n",
    "            img = img.convert(\"L\").convert(\"RGB\")\n",
    "\n",
    "        img = self.resize_and_pad(img)\n",
    "\n",
    "        # RandomCrop n·∫øu mu·ªën\n",
    "        if self.crop_size:\n",
    "            img = T.RandomCrop(self.crop_size)(img)\n",
    "\n",
    "        img = self.to_tensor(img)\n",
    "        img = self.normalize(img)\n",
    "        return img\n",
    "\n",
    "# ------------------- Dataset -------------------\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, content_folder, style_folder, subset,\n",
    "                 transform=None, gray_ratio=0.2,\n",
    "                 valid_ext=('.jpg', '.jpeg', '.png', '.bmp', '.tiff')):\n",
    "        self.content_folder = os.path.join(content_folder, subset)\n",
    "        self.style_folder = os.path.join(style_folder, subset)\n",
    "\n",
    "        self.content_files = []\n",
    "        self.style_files = []\n",
    "\n",
    "        for ext in valid_ext:\n",
    "            self.content_files.extend(glob.glob(os.path.join(self.content_folder, f\"*{ext}\")))\n",
    "            self.style_files.extend(glob.glob(os.path.join(self.style_folder, f\"*{ext}\")))\n",
    "\n",
    "        self.content_files = sorted(self.content_files)\n",
    "        self.style_files = sorted(self.style_files)\n",
    "\n",
    "        if len(self.content_files) == 0:\n",
    "            raise RuntimeError(f\"No content images found in {self.content_folder}\")\n",
    "        if len(self.style_files) == 0:\n",
    "            raise RuntimeError(f\"No style images found in {self.style_folder}\")\n",
    "\n",
    "        self.transform = transform\n",
    "        self.gray_ratio = gray_ratio\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.content_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Content image\n",
    "        content_path = self.content_files[idx]\n",
    "        content_img = Image.open(content_path).convert(\"RGB\")\n",
    "        \n",
    "        # Style image (random)\n",
    "        style_path = random.choice(self.style_files)\n",
    "        style_img = Image.open(style_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            content_img = self.transform(content_img)\n",
    "            style_img = self.transform(style_img)\n",
    "\n",
    "        return content_img, style_img\n",
    "\n",
    "# ------------------- DataLoader factory -------------------\n",
    "def get_dataloaders(content_folder, style_folder,\n",
    "                    batch_size=8, num_workers=4, \n",
    "                    persistent_workers=False, gray_ratio=0.2,\n",
    "                    target_long=512, min_short=256, crop_size=256):\n",
    "    \"\"\"\n",
    "    T·∫°o DataLoader cho train/valid/test.\n",
    "    Gi·∫£ s·ª≠ content_folder v√† style_folder ƒë√£ c√≥ subfolder 'train', 'valid', 'test'.\n",
    "    C√≥ th·ªÉ b·∫≠t tqdm ƒë·ªÉ quan s√°t ti·∫øn tr√¨nh load d·ªØ li·ªáu.\n",
    "    \"\"\"\n",
    "    transform = TransformImageNet(\n",
    "        target_long=target_long,\n",
    "        min_short=min_short,\n",
    "        crop_size=crop_size,\n",
    "        gray_ratio=gray_ratio\n",
    "    )\n",
    "\n",
    "    loaders = {}\n",
    "    for subset in [\"train\", \"valid\", \"test\"]:\n",
    "        dataset = CustomImageDataset(\n",
    "            content_folder,\n",
    "            style_folder,\n",
    "            subset=subset,\n",
    "            transform=transform,\n",
    "            gray_ratio=gray_ratio,\n",
    "        )\n",
    "        shuffle = (subset == \"train\")\n",
    "\n",
    "        loader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=persistent_workers\n",
    "        )\n",
    "\n",
    "        loaders[subset] = loader\n",
    "\n",
    "    return loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb3b856",
   "metadata": {},
   "source": [
    "## AdaIn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53c878c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class VGGEncoder(nn.Module):\n",
    "    def __init__(self, path_vgg_weights=None, device='cpu'):\n",
    "        super(VGGEncoder, self).__init__()\n",
    "        self.device = device  # l∆∞u device ƒë·ªÉ d√πng khi forward\n",
    "        if path_vgg_weights is None:\n",
    "            vgg16 = models.vgg16(pretrained=True)\n",
    "        else:\n",
    "            vgg16 = models.vgg16()\n",
    "            vgg16.load_state_dict(torch.load(path_vgg_weights, map_location=device))\n",
    "\n",
    "        # Ch·ªâ l·∫•y feature t·ªõi conv4_3\n",
    "        self.encoder_layers = nn.Sequential(*list(vgg16.features.children())[:21])\n",
    "\n",
    "        # Freeze weights\n",
    "        for param in self.encoder_layers.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Chuy·ªÉn encoder sang device\n",
    "        self.encoder_layers.to(device)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(self.device)\n",
    "        x = self.encoder_layers(x)\n",
    "        return x\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(channels, channels, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(channels, channels, kernel_size=3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_channels, out_channels, 3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            ResidualBlock(out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, out_channels=3):\n",
    "        super().__init__()\n",
    "        self.decoder_layers = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(512, 256, 3),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            ConvBlock(256, 256),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(256, 256, 3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(256, 128, 3),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            ConvBlock(128, 128),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(128, 64, 3),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            ConvBlock(64, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder_layers(x)\n",
    "\n",
    "    \n",
    "class AdaINet(nn.Module):\n",
    "    def __init__(self, path_vgg_weights=None, out_channels=3, device='cpu'):\n",
    "        super(AdaINet, self).__init__()\n",
    "        self.encoder = VGGEncoder(path_vgg_weights=path_vgg_weights, device=device)\n",
    "        self.decoder = Decoder(out_channels=out_channels)\n",
    "\n",
    "    def forward(self, content, style, alpha=1.0):\n",
    "        # Encode\n",
    "        content_feat = self.encoder(content)\n",
    "        style_feat = self.encoder(style)\n",
    "        # AdaIN\n",
    "        t = self.adain(content_feat, style_feat)\n",
    "        t = alpha * t + (1 - alpha) * content_feat\n",
    "        # Decode\n",
    "        generated = self.decoder(t)\n",
    "        return generated, t\n",
    "\n",
    "    def adain(self, content_feat, style_feat, eps=1e-5):\n",
    "        c_mean = torch.mean(content_feat, dim=[2, 3], keepdim=True)\n",
    "        c_std = torch.std(content_feat, dim=[2, 3], keepdim=True) + eps\n",
    "        s_mean = torch.mean(style_feat, dim=[2, 3], keepdim=True)\n",
    "        s_std = torch.std(style_feat, dim=[2, 3], keepdim=True) + eps\n",
    "\n",
    "        normalized_feat = (content_feat - c_mean) / c_std\n",
    "        stylized_feat = normalized_feat * s_std + s_mean\n",
    "        return stylized_feat\n",
    "\n",
    "    \n",
    "class VGGEncoderMultiLayer(nn.Module):\n",
    "    def __init__(self, path_vgg_weights=None, device='cpu'):\n",
    "        super(VGGEncoderMultiLayer, self).__init__()\n",
    "        self.device = device\n",
    "        if path_vgg_weights is None:\n",
    "            vgg16 = models.vgg16(pretrained=True)\n",
    "        else:\n",
    "            vgg16 = models.vgg16()\n",
    "            vgg16.load_state_dict(torch.load(path_vgg_weights, map_location=device))\n",
    "        \n",
    "        self.slice1 = nn.Sequential(*list(vgg16.features.children())[:2])   # relu1_1\n",
    "        self.slice2 = nn.Sequential(*list(vgg16.features.children())[2:7])  # relu2_1\n",
    "        self.slice3 = nn.Sequential(*list(vgg16.features.children())[7:14]) # relu3_1\n",
    "        self.slice4 = nn.Sequential(*list(vgg16.features.children())[14:21]) # relu4_1\n",
    "        # Freeze weights\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.to(self.device)\n",
    "        relu1_1 = self.slice1(x)\n",
    "        relu2_1 = self.slice2(relu1_1)\n",
    "        relu3_1 = self.slice3(relu2_1)\n",
    "        relu4_1 = self.slice4(relu3_1)\n",
    "        return {\n",
    "            'relu1_1': relu1_1,\n",
    "            'relu2_1': relu2_1,\n",
    "            'relu3_1': relu3_1,\n",
    "            'relu4_1': relu4_1\n",
    "        }\n",
    "class AdaINLossMultiLayer(nn.Module):\n",
    "    def __init__(self, encoder, alpha=1.0, beta=0.5, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.eps = eps\n",
    "        self.style_layers = ['relu1_1', 'relu2_1', 'relu3_1', 'relu4_1']\n",
    "\n",
    "    def calc_mean_std(self, feat):\n",
    "        B, C = feat.size(0), feat.size(1)\n",
    "        feat_reshaped = feat.view(B, C, -1)\n",
    "        mean = feat_reshaped.mean(dim=2).view(B, C, 1, 1)\n",
    "        std = feat_reshaped.std(dim=2).view(B, C, 1, 1) + self.eps\n",
    "        return mean, std\n",
    "\n",
    "    def content_loss(self, gen_feat, t):\n",
    "        return nn.functional.mse_loss(gen_feat, t)\n",
    "\n",
    "    def style_loss(self, gen_feat, style_feat):\n",
    "        loss = 0.0\n",
    "        for layer in self.style_layers:\n",
    "            g_mean, g_std = self.calc_mean_std(gen_feat[layer])\n",
    "            s_mean, s_std = self.calc_mean_std(style_feat[layer])\n",
    "            loss += nn.functional.mse_loss(g_mean, s_mean) + nn.functional.mse_loss(g_std, s_std)\n",
    "        return loss\n",
    "\n",
    "    def forward(self, generated, t, style):\n",
    "        gen_feat = self.encoder(generated)\n",
    "        style_feat = self.encoder(style)\n",
    "        # resize t cho kh·ªõp v·ªõi gen_feat\n",
    "        t_resized = nn.functional.interpolate(t, size=gen_feat['relu4_1'].shape[2:], mode='nearest')\n",
    "        c_loss = self.content_loss(gen_feat['relu4_1'], t_resized)\n",
    "        s_loss = self.style_loss(gen_feat, style_feat)\n",
    "\n",
    "        total = self.alpha * c_loss + self.beta * s_loss\n",
    "        return total, c_loss, s_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a92127f",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70084751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "def train_model(\n",
    "    train_loader, test_loader,\n",
    "    model, criterion, optimizer,\n",
    "    device, num_epochs,\n",
    "    save_dir=\"./checkpoints\"\n",
    "):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # ====================== LOAD CHECKPOINT ======================\n",
    "    checkpoint_path = os.path.join(save_dir, \"latest_checkpoint.pth\")\n",
    "    start_epoch = 0\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"üîÑ Loading checkpoint from {checkpoint_path}\")\n",
    "        ckpt = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(ckpt[\"model\"])\n",
    "        optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
    "        start_epoch = ckpt[\"epoch\"] + 1\n",
    "        best_val_loss = ckpt[\"best_val_loss\"]\n",
    "        print(f\"‚û° Continue from epoch {start_epoch}, best_val_loss={best_val_loss:.4f}\")\n",
    "\n",
    "    # ====================== TRAIN LOOP ======================\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch} Training\")\n",
    "\n",
    "        for content, style in pbar:\n",
    "            content, style = content.to(device), style.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            generated, t = model(content, style)\n",
    "            loss, c_loss, s_loss = criterion(generated, t, style)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            pbar.set_postfix({\n",
    "                \"Loss\": f\"{loss.item():.4f}\",\n",
    "                \"Content\": f\"{c_loss.item():.4f}\",\n",
    "                \"Style\": f\"{s_loss.item():.4f}\",\n",
    "            })\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "\n",
    "        # ====================== VALIDATION ======================\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        sample_saved = False\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (content, style) in enumerate(test_loader):\n",
    "                content, style = content.to(device), style.to(device)\n",
    "                generated, target_feat = model(content, style)\n",
    "                loss, _, _ = criterion(generated, target_feat, style)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                if not sample_saved:\n",
    "                    img_path = os.path.join(save_dir, f\"epoch_{epoch}_sample.png\")\n",
    "                    save_image(generated.clamp(0,1), img_path)\n",
    "                    sample_saved = True\n",
    "\n",
    "        avg_val_loss = val_loss / len(test_loader)\n",
    "        print(f\"‚úÖ Epoch {epoch} | Train: {avg_train_loss:.4f} | Val: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # ====================== SAVE BEST MODEL ======================\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_path = os.path.join(save_dir, \"best_model.pth\")\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "            print(f\"üèÜ Best model updated! Saved: {best_path}\")\n",
    "\n",
    "        # ====================== SAVE CHECKPOINT (Resume) ======================\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"best_val_loss\": best_val_loss,\n",
    "        }, checkpoint_path)\n",
    "\n",
    "    print(\"üéØ Training Completed!\")\n",
    "    return best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089668c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as F\n",
    "loader = get_dataloaders(content_folder=\"../data/coco2017\",\n",
    "                         style_folder=\"../data/wikiart_sampled\", num_workers=32, batch_size=16, persistent_workers=False)\n",
    "path_vgg_weights = \"../models/vgg16-encoder.pth\"\n",
    "device = (\"cuda:7\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = AdaINLossMultiLayer(encoder=VGGEncoderMultiLayer(path_vgg_weights=path_vgg_weights, device=device), alpha=1.0, beta=10)\n",
    "model = AdaINet(out_channels=3, path_vgg_weights=path_vgg_weights, device=device).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4*0.5)\n",
    "best_val_loss = train_model(\n",
    "    train_loader=loader[\"train\"],\n",
    "    test_loader=loader[\"valid\"],\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=100,\n",
    "    save_dir=\"checkpoints_adaln\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece9813d",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07449a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os, glob, random\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def load_image(path, device):\n",
    "    \"\"\"Load 1 ·∫£nh, resize v√† chuy·ªÉn th√†nh tensor [1,3,H,W]\"\"\"\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "    ])\n",
    "    img = transform(img).unsqueeze(0).to(device)\n",
    "    return img\n",
    "\n",
    "def smooth_image(tensor):\n",
    "    \"\"\"tensor: [3,H,W] trong range [0,1]\"\"\"\n",
    "    img = tensor.detach().cpu().permute(1,2,0).numpy()\n",
    "    img = (img * 255).astype(np.uint8)\n",
    "    img = cv2.bilateralFilter(img, d=7, sigmaColor=50, sigmaSpace=50)\n",
    "    img = torch.tensor(img.astype(np.float32)/255).permute(2,0,1)\n",
    "    return img\n",
    "\n",
    "def unnormalize(tensor):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406], device=tensor.device).view(1, 3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225], device=tensor.device).view(1, 3, 1, 1)\n",
    "    return tensor * std + mean\n",
    "\n",
    "def test_model(model, content_dir, style_dir, device, checkpoint_path, save_dir=\"./test_results\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    content_paths = sorted(glob.glob(os.path.join(content_dir, \"*.jpg\")))\n",
    "    style_paths = sorted(glob.glob(os.path.join(style_dir, \"*.jpg\")))\n",
    "    content_paths = content_paths[:20]\n",
    "\n",
    "    # Load checkpoint\n",
    "    print(f\"üîÑ Loading model weights from {checkpoint_path}\")\n",
    "    ckpt = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(ckpt)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for ci, content_path in enumerate(tqdm(content_paths, desc=\"Testing content\")):\n",
    "            # üëâ ch·ªçn ng·∫´u nhi√™n 1 style cho m·ªói content\n",
    "            style_path = random.choice(style_paths)\n",
    "\n",
    "            # Load ·∫£nh\n",
    "            content = load_image(content_path, device)\n",
    "            style = load_image(style_path, device)\n",
    "\n",
    "            # Save k·∫øt qu·∫£\n",
    "            pair_dir = os.path.join(save_dir, f\"pair_{ci + 1}\")\n",
    "            os.makedirs(pair_dir, exist_ok=True)\n",
    "\n",
    "            save_image(unnormalize(content).clamp(0, 1),\n",
    "                       os.path.join(pair_dir, \"content.jpg\"))\n",
    "            save_image(unnormalize(style).clamp(0, 1),\n",
    "                       os.path.join(pair_dir, \"style.jpg\"))\n",
    "            for alpha in [0.8, 1.0, 1.2]:\n",
    "                generated, _ = model(content, style, alpha=alpha)\n",
    "\n",
    "                # Resize output v·ªÅ ƒë√∫ng k√≠ch th∆∞·ªõc content\n",
    "                _, _, h, w = content.shape\n",
    "                resized = F.interpolate(generated, size=(h, w), mode='bilinear', align_corners=False)\n",
    "                out = unnormalize(resized)\n",
    "                # Save\n",
    "                out_path = os.path.join(pair_dir, f\"result_alpha_{alpha:.1f}.jpg\")\n",
    "                save_image(out.clamp(0,1), out_path)\n",
    "\n",
    "    print(f\"‚úÖ Testing completed! Results saved in {save_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e21627bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading model weights from ./checkpoints_adaln/best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing content: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:12<00:00,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Testing completed! Results saved in ../results/adain\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "path_vgg_weights = \"../models/vgg16-encoder.pth\"\n",
    "device = (\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AdaINet(out_channels=3, path_vgg_weights=path_vgg_weights, device=device).to(device)\n",
    "test_model(\n",
    "    model=model,\n",
    "    content_dir=\"../data/coco2017/test\",\n",
    "    style_dir=\"../data/wikiart_sampled/test\",\n",
    "    device=device,\n",
    "    checkpoint_path=\"./checkpoints_adaln/best_model.pth\",\n",
    "    save_dir=\"../results/adain\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
