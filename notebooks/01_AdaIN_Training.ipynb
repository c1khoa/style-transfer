{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a824e49f",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3467b361",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as F\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import shutil\n",
    "import random\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def style_sampling(base_path, dest_path, n_samples=100, split=(0.8, 0.1, 0.1)):\n",
    "    \"\"\"\n",
    "    L·∫•y m·∫´u ·∫£nh t·ª´ m·ªói style v√† chia ƒë·ªÅu train/valid/test.\n",
    "    ƒê·∫∑t t√™n file d·∫°ng style_001.jpg, style_002.jpg...\n",
    "    \n",
    "    Args:\n",
    "        base_path (str): th∆∞ m·ª•c ch·ª©a c√°c folder style\n",
    "        dest_path (str): th∆∞ m·ª•c l∆∞u k·∫øt qu·∫£\n",
    "        n_samples (int): s·ªë ·∫£nh l·∫•y m·∫´u cho m·ªói style\n",
    "        split (tuple): t·ªâ l·ªá chia train/valid/test\n",
    "    \"\"\"\n",
    "    random.seed(2025)\n",
    "    \n",
    "    subsets = [\"train\", \"valid\", \"test\"]\n",
    "    for subset in subsets:\n",
    "        os.makedirs(os.path.join(dest_path, subset), exist_ok=True)\n",
    "    \n",
    "    styles = [d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))]\n",
    "    total_copied = 0\n",
    "\n",
    "    for style in tqdm(styles, desc=\"Sampling styles\"):\n",
    "        style_path = os.path.join(base_path, style)\n",
    "        # L·ªçc file ·∫£nh\n",
    "        image_files = [f for f in os.listdir(style_path) \n",
    "                       if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff'))]\n",
    "        if not image_files:\n",
    "            continue\n",
    "\n",
    "        # L·∫•y m·∫´u ng·∫´u nhi√™n\n",
    "        sample_imgs = random.sample(image_files, min(len(image_files), n_samples))\n",
    "        n = len(sample_imgs)\n",
    "\n",
    "        # S·ªë l∆∞·ª£ng cho train/valid/test (l√†m tr√≤n xu·ªëng)\n",
    "        n_train = int(n * split[0])\n",
    "        n_valid = int(n * split[1])\n",
    "        n_test = n - n_train - n_valid\n",
    "\n",
    "        # Chia ·∫£nh\n",
    "        split_dict = {\n",
    "            \"train\": sample_imgs[:n_train],\n",
    "            \"valid\": sample_imgs[n_train:n_train+n_valid],\n",
    "            \"test\": sample_imgs[n_train+n_valid:]\n",
    "        }\n",
    "\n",
    "        # Copy ·∫£nh sang folder t∆∞∆°ng ·ª©ng v·ªõi t√™n style_index.jpg\n",
    "        for subset, imgs in split_dict.items():\n",
    "            dest_folder = os.path.join(dest_path, subset)\n",
    "            for idx, img_name in enumerate(imgs, start=1):\n",
    "                src = os.path.join(style_path, img_name)\n",
    "                dst_name = f\"{style}_{idx:03d}.jpg\"  # style_001.jpg, style_002.jpg...\n",
    "                dst = os.path.join(dest_folder, dst_name)\n",
    "                shutil.copy(src, dst)\n",
    "                total_copied += 1\n",
    "\n",
    "    print(f\"ƒê√£ sao ch√©p {total_copied} ·∫£nh t·ª´ {len(styles)} style v√†o {dest_path}/train, valid, test\")\n",
    "\n",
    "\n",
    "# ------------------- Transform -------------------\n",
    "class TransformImageNet:\n",
    "    def __init__(self, target_long=512, min_short=256, crop_size=None, gray_ratio=0.0):\n",
    "        \"\"\"\n",
    "        target_long: c·∫°nh l·ªõn c·ªßa ·∫£nh sau resize\n",
    "        min_short: n·∫øu c·∫°nh nh·ªè < min_short, s·∫Ω padding\n",
    "        crop_size: n·∫øu mu·ªën crop, None = kh√¥ng crop\n",
    "        gray_ratio: x√°c su·∫•t chuy·ªÉn ·∫£nh sang grayscale\n",
    "        \"\"\"\n",
    "        self.target_long = target_long\n",
    "        self.min_short = min_short\n",
    "        self.crop_size = crop_size\n",
    "        self.gray_ratio = gray_ratio\n",
    "        self.to_tensor = T.ToTensor()\n",
    "        self.normalize = T.Normalize(mean=[0.485,0.456,0.406],\n",
    "                                     std=[0.229,0.224,0.225])\n",
    "\n",
    "    def resize_and_pad(self, img):\n",
    "        w, h = img.size\n",
    "        # scale t·ªâ l·ªá theo c·∫°nh l·ªõn\n",
    "        if w > h:\n",
    "            new_w = self.target_long\n",
    "            new_h = int(h * self.target_long / w)\n",
    "        else:\n",
    "            new_h = self.target_long\n",
    "            new_w = int(w * self.target_long / h)\n",
    "        img = img.resize((new_w, new_h), Image.Resampling.LANCZOS)\n",
    "\n",
    "        # padding n·∫øu c·∫°nh nh·ªè < min_short\n",
    "        pad_w = max(0, self.min_short - new_w)\n",
    "        pad_h = max(0, self.min_short - new_h)\n",
    "        if pad_w > 0 or pad_h > 0:\n",
    "            img = F.pad(img, (0,0,pad_w,pad_h), fill=0)\n",
    "        return img\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # Grayscale augmentation\n",
    "        if random.random() < self.gray_ratio:\n",
    "            img = img.convert(\"L\").convert(\"RGB\")\n",
    "\n",
    "        img = self.resize_and_pad(img)\n",
    "\n",
    "        # RandomCrop n·∫øu mu·ªën\n",
    "        if self.crop_size:\n",
    "            img = T.RandomCrop(self.crop_size)(img)\n",
    "\n",
    "        img = self.to_tensor(img)\n",
    "        img = self.normalize(img)\n",
    "        return img\n",
    "\n",
    "# ------------------- Dataset -------------------\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, content_folder, style_folder, subset,\n",
    "                 transform=None, gray_ratio=0.2,\n",
    "                 valid_ext=('.jpg', '.jpeg', '.png', '.bmp', '.tiff')):\n",
    "        self.content_folder = os.path.join(content_folder, subset)\n",
    "        self.style_folder = os.path.join(style_folder, subset)\n",
    "\n",
    "        self.content_files = []\n",
    "        self.style_files = []\n",
    "\n",
    "        for ext in valid_ext:\n",
    "            self.content_files.extend(glob.glob(os.path.join(self.content_folder, f\"*{ext}\")))\n",
    "            self.style_files.extend(glob.glob(os.path.join(self.style_folder, f\"*{ext}\")))\n",
    "\n",
    "        self.content_files = sorted(self.content_files)\n",
    "        self.style_files = sorted(self.style_files)\n",
    "\n",
    "        if len(self.content_files) == 0:\n",
    "            raise RuntimeError(f\"No content images found in {self.content_folder}\")\n",
    "        if len(self.style_files) == 0:\n",
    "            raise RuntimeError(f\"No style images found in {self.style_folder}\")\n",
    "\n",
    "        self.transform = transform\n",
    "        self.gray_ratio = gray_ratio\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.content_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Content image\n",
    "        content_path = self.content_files[idx]\n",
    "        content_img = Image.open(content_path).convert(\"RGB\")\n",
    "        \n",
    "        # Style image (random)\n",
    "        style_path = random.choice(self.style_files)\n",
    "        style_img = Image.open(style_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            content_img = self.transform(content_img)\n",
    "            style_img = self.transform(style_img)\n",
    "\n",
    "        return content_img, style_img\n",
    "\n",
    "# ------------------- DataLoader factory -------------------\n",
    "def get_dataloaders(content_folder, style_folder,\n",
    "                    batch_size=8, num_workers=4, \n",
    "                    persistent_workers=False, gray_ratio=0.2,\n",
    "                    target_long=512, min_short=256, crop_size=256):\n",
    "    \"\"\"\n",
    "    T·∫°o DataLoader cho train/valid/test.\n",
    "    Gi·∫£ s·ª≠ content_folder v√† style_folder ƒë√£ c√≥ subfolder 'train', 'valid', 'test'.\n",
    "    C√≥ th·ªÉ b·∫≠t tqdm ƒë·ªÉ quan s√°t ti·∫øn tr√¨nh load d·ªØ li·ªáu.\n",
    "    \"\"\"\n",
    "    transform = TransformImageNet(\n",
    "        target_long=target_long,\n",
    "        min_short=min_short,\n",
    "        crop_size=crop_size,\n",
    "        gray_ratio=gray_ratio\n",
    "    )\n",
    "\n",
    "    loaders = {}\n",
    "    for subset in [\"train\", \"valid\", \"test\"]:\n",
    "        dataset = CustomImageDataset(\n",
    "            content_folder,\n",
    "            style_folder,\n",
    "            subset=subset,\n",
    "            transform=transform,\n",
    "            gray_ratio=gray_ratio,\n",
    "        )\n",
    "        shuffle = (subset == \"train\")\n",
    "\n",
    "        loader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=persistent_workers\n",
    "        )\n",
    "\n",
    "        loaders[subset] = loader\n",
    "\n",
    "    return loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb3b856",
   "metadata": {},
   "source": [
    "## AdaIn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53c878c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class VGGEncoder(nn.Module):\n",
    "    def __init__(self, path_vgg_weights=None, device='cpu'):\n",
    "        super(VGGEncoder, self).__init__()\n",
    "        self.device = device  # l∆∞u device ƒë·ªÉ d√πng khi forward\n",
    "        if path_vgg_weights is None:\n",
    "            vgg19 = models.vgg19(pretrained=True)\n",
    "        else:\n",
    "            vgg19 = models.vgg19()\n",
    "            vgg19.load_state_dict(torch.load(path_vgg_weights, map_location=device))\n",
    "\n",
    "        # Ch·ªâ l·∫•y feature t·ªõi conv4_3\n",
    "        self.encoder_layers = nn.Sequential(*list(vgg19.features.children())[:21])\n",
    "\n",
    "        # Freeze weights\n",
    "        for param in self.encoder_layers.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Chuy·ªÉn encoder sang device\n",
    "        self.encoder_layers.to(device)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(self.device)\n",
    "        x = self.encoder_layers(x)\n",
    "        return x\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(channels, channels, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(channels, channels, kernel_size=3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_channels, out_channels, 3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            ResidualBlock(out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, out_channels=3):\n",
    "        super().__init__()\n",
    "        self.decoder_layers = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(512, 256, 3),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            ConvBlock(256, 256),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(256, 256, 3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(256, 128, 3),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            ConvBlock(128, 128),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(128, 64, 3),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            ConvBlock(64, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder_layers(x)\n",
    "\n",
    "    \n",
    "class AdaINet(nn.Module):\n",
    "    def __init__(self, path_vgg_weights=None, out_channels=3, device='cpu'):\n",
    "        super(AdaINet, self).__init__()\n",
    "        self.encoder = VGGEncoder(path_vgg_weights=path_vgg_weights, device=device)\n",
    "        self.decoder = Decoder(out_channels=out_channels)\n",
    "\n",
    "    def forward(self, content, style, alpha=1.0):\n",
    "        # Encode\n",
    "        content_feat = self.encoder(content)\n",
    "        style_feat = self.encoder(style)\n",
    "        # AdaIN\n",
    "        t = self.adain(content_feat, style_feat)\n",
    "        t = alpha * t + (1 - alpha) * content_feat\n",
    "        # Decode\n",
    "        generated = self.decoder(t)\n",
    "        return generated, t\n",
    "\n",
    "    def adain(self, content_feat, style_feat, eps=1e-5):\n",
    "        c_mean = torch.mean(content_feat, dim=[2, 3], keepdim=True)\n",
    "        c_std = torch.std(content_feat, dim=[2, 3], keepdim=True) + eps\n",
    "        s_mean = torch.mean(style_feat, dim=[2, 3], keepdim=True)\n",
    "        s_std = torch.std(style_feat, dim=[2, 3], keepdim=True) + eps\n",
    "\n",
    "        normalized_feat = (content_feat - c_mean) / c_std\n",
    "        stylized_feat = normalized_feat * s_std + s_mean\n",
    "        return stylized_feat\n",
    "\n",
    "    \n",
    "class VGGEncoderMultiLayer(nn.Module):\n",
    "    def __init__(self, path_vgg_weights=None, device='cpu'):\n",
    "        super(VGGEncoderMultiLayer, self).__init__()\n",
    "        self.device = device\n",
    "        if path_vgg_weights is None:\n",
    "            vgg19 = models.vgg19(pretrained=True)\n",
    "        else:\n",
    "            vgg19 = models.vgg19()\n",
    "            vgg19.load_state_dict(torch.load(path_vgg_weights, map_location=device))\n",
    "        \n",
    "        self.slice1 = nn.Sequential(*list(vgg19.features.children())[:2])   # relu1_1\n",
    "        self.slice2 = nn.Sequential(*list(vgg19.features.children())[2:7])  # relu2_1\n",
    "        self.slice3 = nn.Sequential(*list(vgg19.features.children())[7:14]) # relu3_1\n",
    "        self.slice4 = nn.Sequential(*list(vgg19.features.children())[14:21]) # relu4_1\n",
    "        # Freeze weights\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.to(self.device)\n",
    "        relu1_1 = self.slice1(x)\n",
    "        relu2_1 = self.slice2(relu1_1)\n",
    "        relu3_1 = self.slice3(relu2_1)\n",
    "        relu4_1 = self.slice4(relu3_1)\n",
    "        return {\n",
    "            'relu1_1': relu1_1,\n",
    "            'relu2_1': relu2_1,\n",
    "            'relu3_1': relu3_1,\n",
    "            'relu4_1': relu4_1\n",
    "        }\n",
    "class AdaINLossMultiLayer(nn.Module):\n",
    "    def __init__(self, encoder, alpha=1.0, beta=0.5, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.eps = eps\n",
    "        self.style_layers = ['relu1_1', 'relu2_1', 'relu3_1', 'relu4_1']\n",
    "\n",
    "    def calc_mean_std(self, feat):\n",
    "        B, C = feat.size(0), feat.size(1)\n",
    "        feat_reshaped = feat.view(B, C, -1)\n",
    "        mean = feat_reshaped.mean(dim=2).view(B, C, 1, 1)\n",
    "        std = feat_reshaped.std(dim=2).view(B, C, 1, 1) + self.eps\n",
    "        return mean, std\n",
    "\n",
    "    def content_loss(self, gen_feat, t):\n",
    "        return nn.functional.mse_loss(gen_feat, t)\n",
    "\n",
    "    def style_loss(self, gen_feat, style_feat):\n",
    "        loss = 0.0\n",
    "        for layer in self.style_layers:\n",
    "            g_mean, g_std = self.calc_mean_std(gen_feat[layer])\n",
    "            s_mean, s_std = self.calc_mean_std(style_feat[layer])\n",
    "            loss += nn.functional.mse_loss(g_mean, s_mean) + nn.functional.mse_loss(g_std, s_std)\n",
    "        return loss\n",
    "\n",
    "    def forward(self, generated, t, style):\n",
    "        gen_feat = self.encoder(generated)\n",
    "        style_feat = self.encoder(style)\n",
    "        # resize t cho kh·ªõp v·ªõi gen_feat\n",
    "        t_resized = nn.functional.interpolate(t, size=gen_feat['relu4_1'].shape[2:], mode='nearest')\n",
    "        c_loss = self.content_loss(gen_feat['relu4_1'], t_resized)\n",
    "        s_loss = self.style_loss(gen_feat, style_feat)\n",
    "\n",
    "        total = self.alpha * c_loss + self.beta * s_loss\n",
    "        return total, c_loss, s_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a92127f",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70084751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "def train_model(\n",
    "    train_loader, test_loader,\n",
    "    model, criterion, optimizer,\n",
    "    device, num_epochs,\n",
    "    save_dir=\"./checkpoints\"\n",
    "):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # ====================== LOAD CHECKPOINT ======================\n",
    "    checkpoint_path = os.path.join(save_dir, \"latest_checkpoint.pth\")\n",
    "    start_epoch = 0\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"üîÑ Loading checkpoint from {checkpoint_path}\")\n",
    "        ckpt = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(ckpt[\"model\"])\n",
    "        optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
    "        start_epoch = ckpt[\"epoch\"] + 1\n",
    "        best_val_loss = ckpt[\"best_val_loss\"]\n",
    "        print(f\"‚û° Continue from epoch {start_epoch}, best_val_loss={best_val_loss:.4f}\")\n",
    "\n",
    "    # ====================== TRAIN LOOP ======================\n",
    "    for epoch in range(start_epoch + 1, num_epochs + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch} Training\")\n",
    "\n",
    "        for content, style in pbar:\n",
    "            content, style = content.to(device), style.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            generated, t = model(content, style)\n",
    "            loss, c_loss, s_loss = criterion(generated, t, style)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            pbar.set_postfix({\n",
    "                \"Loss\": f\"{loss.item():.4f}\",\n",
    "                \"Content\": f\"{c_loss.item():.4f}\",\n",
    "                \"Style\": f\"{s_loss.item():.4f}\",\n",
    "            })\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "\n",
    "        # ====================== VALIDATION ======================\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        sample_saved = False\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (content, style) in enumerate(test_loader):\n",
    "                content, style = content.to(device), style.to(device)\n",
    "                generated, target_feat = model(content, style)\n",
    "                loss, _, _ = criterion(generated, target_feat, style)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                if not sample_saved:\n",
    "                    img_path = os.path.join(save_dir, f\"epoch_{epoch}_sample.png\")\n",
    "                    save_image(generated.clamp(0,1), img_path)\n",
    "                    sample_saved = True\n",
    "\n",
    "        avg_val_loss = val_loss / len(test_loader)\n",
    "        print(f\"‚úÖ Epoch {epoch} | Train: {avg_train_loss:.4f} | Val: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # ====================== SAVE BEST MODEL ======================\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_path = os.path.join(save_dir, \"best_model.pth\")\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "            print(f\"üèÜ Best model updated! Saved: {best_path}\")\n",
    "\n",
    "        # ====================== SAVE CHECKPOINT (Resume) ======================\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"best_val_loss\": best_val_loss,\n",
    "        }, checkpoint_path)\n",
    "\n",
    "    print(\"üéØ Training Completed!\")\n",
    "    return best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "089668c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7393/7393 [44:36<00:00,  2.76it/s, Loss=27.0191, Content=12.3609, Style=1.4658]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 1 | Train: 42.3589 | Val: 27.9994\n",
      "üèÜ Best model updated! Saved: checkpoints_adaln/best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7393/7393 [43:55<00:00,  2.81it/s, Loss=21.6986, Content=9.8964, Style=1.1802] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 2 | Train: 26.9479 | Val: 23.7101\n",
      "üèÜ Best model updated! Saved: checkpoints_adaln/best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7393/7393 [44:30<00:00,  2.77it/s, Loss=20.3479, Content=10.5261, Style=0.9822]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 3 | Train: 23.7721 | Val: 22.0815\n",
      "üèÜ Best model updated! Saved: checkpoints_adaln/best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7393/7393 [43:41<00:00,  2.82it/s, Loss=24.7104, Content=12.1015, Style=1.2609]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 4 | Train: 22.0859 | Val: 20.5901\n",
      "üèÜ Best model updated! Saved: checkpoints_adaln/best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7393/7393 [44:30<00:00,  2.77it/s, Loss=23.9760, Content=11.9395, Style=1.2036]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 5 | Train: 20.9490 | Val: 19.6499\n",
      "üèÜ Best model updated! Saved: checkpoints_adaln/best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7393/7393 [41:57<00:00,  2.94it/s, Loss=16.4754, Content=8.1747, Style=0.8301] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 6 | Train: 20.1510 | Val: 19.2875\n",
      "üèÜ Best model updated! Saved: checkpoints_adaln/best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7393/7393 [42:07<00:00,  2.93it/s, Loss=25.0993, Content=12.3067, Style=1.2793]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 7 | Train: 19.5458 | Val: 19.0018\n",
      "üèÜ Best model updated! Saved: checkpoints_adaln/best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7393/7393 [42:35<00:00,  2.89it/s, Loss=19.4890, Content=9.5534, Style=0.9936] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 8 | Train: 18.9838 | Val: 18.2381\n",
      "üèÜ Best model updated! Saved: checkpoints_adaln/best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7393/7393 [41:43<00:00,  2.95it/s, Loss=17.0811, Content=8.9079, Style=0.8173] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 9 | Train: 18.6621 | Val: 18.2244\n",
      "üèÜ Best model updated! Saved: checkpoints_adaln/best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7393/7393 [41:47<00:00,  2.95it/s, Loss=20.6248, Content=10.2450, Style=1.0380]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 10 | Train: 18.3973 | Val: 17.8170\n",
      "üèÜ Best model updated! Saved: checkpoints_adaln/best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7393/7393 [41:52<00:00,  2.94it/s, Loss=17.3373, Content=8.8371, Style=0.8500] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 11 | Train: 18.0825 | Val: 17.6929\n",
      "üèÜ Best model updated! Saved: checkpoints_adaln/best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7393/7393 [42:14<00:00,  2.92it/s, Loss=22.4544, Content=11.4177, Style=1.1037]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 12 | Train: 17.8827 | Val: 17.3491\n",
      "üèÜ Best model updated! Saved: checkpoints_adaln/best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7393/7393 [41:20<00:00,  2.98it/s, Loss=14.9834, Content=7.8763, Style=0.7107] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 13 | Train: 17.6824 | Val: 17.3186\n",
      "üèÜ Best model updated! Saved: checkpoints_adaln/best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7393/7393 [41:32<00:00,  2.97it/s, Loss=14.9529, Content=7.8193, Style=0.7134] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 14 | Train: 17.5745 | Val: 17.7008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7393/7393 [42:14<00:00,  2.92it/s, Loss=16.9495, Content=8.7697, Style=0.8180] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 15 | Train: 17.3898 | Val: 17.1051\n",
      "üèÜ Best model updated! Saved: checkpoints_adaln/best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7393/7393 [43:58<00:00,  2.80it/s, Loss=23.9362, Content=11.7325, Style=1.2204]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 16 | Train: 17.2330 | Val: 16.8057\n",
      "üèÜ Best model updated! Saved: checkpoints_adaln/best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7393/7393 [43:26<00:00,  2.84it/s, Loss=19.5505, Content=10.1435, Style=0.9407]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 17 | Train: 17.0967 | Val: 16.9358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7393/7393 [43:21<00:00,  2.84it/s, Loss=12.9779, Content=6.7530, Style=0.6225] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 18 | Train: 17.0519 | Val: 16.6971\n",
      "üèÜ Best model updated! Saved: checkpoints_adaln/best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7393/7393 [44:07<00:00,  2.79it/s, Loss=14.4173, Content=6.5595, Style=0.7858] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 19 | Train: 16.8844 | Val: 16.8819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7393/7393 [42:45<00:00,  2.88it/s, Loss=16.9671, Content=8.4809, Style=0.8486] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 20 | Train: 16.8295 | Val: 16.7640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7393/7393 [42:03<00:00,  2.93it/s, Loss=14.8463, Content=7.9576, Style=0.6889] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 21 | Train: 16.6649 | Val: 16.2724\n",
      "üèÜ Best model updated! Saved: checkpoints_adaln/best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7393/7393 [42:53<00:00,  2.87it/s, Loss=14.6827, Content=8.0460, Style=0.6637] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 22 | Train: 16.5924 | Val: 16.2699\n",
      "üèÜ Best model updated! Saved: checkpoints_adaln/best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7393/7393 [42:51<00:00,  2.87it/s, Loss=10.5540, Content=5.4513, Style=0.5103] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 23 | Train: 16.6026 | Val: 16.3367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7393/7393 [41:21<00:00,  2.98it/s, Loss=17.0318, Content=9.0178, Style=0.8014]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 24 | Train: 16.4983 | Val: 16.2744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7393/7393 [42:31<00:00,  2.90it/s, Loss=16.7971, Content=8.7661, Style=0.8031] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 25 | Train: 16.3984 | Val: 16.1331\n",
      "üèÜ Best model updated! Saved: checkpoints_adaln/best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7393/7393 [42:29<00:00,  2.90it/s, Loss=22.0356, Content=9.5426, Style=1.2493] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 26 | Train: 16.3901 | Val: 15.9972\n",
      "üèÜ Best model updated! Saved: checkpoints_adaln/best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7393/7393 [42:52<00:00,  2.87it/s, Loss=14.8152, Content=7.2599, Style=0.7555] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 27 | Train: 16.3241 | Val: 16.0570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7393/7393 [43:32<00:00,  2.83it/s, Loss=15.1826, Content=8.0524, Style=0.7130] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 28 | Train: 16.2939 | Val: 16.0063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7393/7393 [42:47<00:00,  2.88it/s, Loss=16.3475, Content=8.6288, Style=0.7719] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 29 | Train: 16.1992 | Val: 15.6524\n",
      "üèÜ Best model updated! Saved: checkpoints_adaln/best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7393/7393 [42:08<00:00,  2.92it/s, Loss=16.3666, Content=8.2768, Style=0.8090] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 30 | Train: 16.1050 | Val: 15.9236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7393/7393 [42:31<00:00,  2.90it/s, Loss=16.6719, Content=8.1018, Style=0.8570] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 31 | Train: 16.0638 | Val: 15.6007\n",
      "üèÜ Best model updated! Saved: checkpoints_adaln/best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7393/7393 [42:06<00:00,  2.93it/s, Loss=13.1379, Content=6.2882, Style=0.6850] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 32 | Train: 16.0770 | Val: 15.7848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7393/7393 [42:28<00:00,  2.90it/s, Loss=13.8813, Content=7.4583, Style=0.6423] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 33 | Train: 15.9968 | Val: 15.6326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7393/7393 [42:19<00:00,  2.91it/s, Loss=14.7660, Content=7.6256, Style=0.7140] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 34 | Train: 15.9689 | Val: 15.9310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7393/7393 [43:52<00:00,  2.81it/s, Loss=13.4401, Content=6.9956, Style=0.6444] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 35 | Train: 15.9501 | Val: 15.5572\n",
      "üèÜ Best model updated! Saved: checkpoints_adaln/best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7393/7393 [44:34<00:00,  2.76it/s, Loss=12.8020, Content=6.4856, Style=0.6316] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 36 | Train: 15.8927 | Val: 16.0707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7393/7393 [44:34<00:00,  2.76it/s, Loss=11.8183, Content=6.1942, Style=0.5624] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 37 | Train: 15.8000 | Val: 15.8556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7393/7393 [44:01<00:00,  2.80it/s, Loss=14.2297, Content=7.1689, Style=0.7061] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 38 | Train: 15.7980 | Val: 15.3429\n",
      "üèÜ Best model updated! Saved: checkpoints_adaln/best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7393/7393 [42:31<00:00,  2.90it/s, Loss=16.1509, Content=8.5662, Style=0.7585] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 39 | Train: 15.7719 | Val: 15.5238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7393/7393 [41:17<00:00,  2.98it/s, Loss=18.9347, Content=10.0059, Style=0.8929]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 40 | Train: 15.7473 | Val: 15.4320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7393/7393 [41:48<00:00,  2.95it/s, Loss=13.1887, Content=6.8703, Style=0.6318] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 41 | Train: 15.7579 | Val: 15.0980\n",
      "üèÜ Best model updated! Saved: checkpoints_adaln/best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7393/7393 [42:22<00:00,  2.91it/s, Loss=26.2230, Content=9.6620, Style=1.6561] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 42 | Train: 15.6658 | Val: 15.5563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7393/7393 [42:08<00:00,  2.92it/s, Loss=11.7257, Content=5.6413, Style=0.6084] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 43 | Train: 15.6601 | Val: 15.6823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7393/7393 [42:20<00:00,  2.91it/s, Loss=17.4502, Content=9.5318, Style=0.7918] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 44 | Train: 15.6549 | Val: 15.5584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7393/7393 [42:41<00:00,  2.89it/s, Loss=17.3954, Content=9.0403, Style=0.8355] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 45 | Train: 15.5627 | Val: 15.6663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7393/7393 [41:14<00:00,  2.99it/s, Loss=15.4773, Content=7.7898, Style=0.7687] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 46 | Train: 15.6199 | Val: 15.3248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7393/7393 [41:30<00:00,  2.97it/s, Loss=14.7440, Content=7.3743, Style=0.7370] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 47 | Train: 15.5511 | Val: 15.4204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7393/7393 [41:00<00:00,  3.00it/s, Loss=14.0261, Content=6.9587, Style=0.7067] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 48 | Train: 15.5232 | Val: 15.2205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7393/7393 [42:27<00:00,  2.90it/s, Loss=15.5791, Content=7.7439, Style=0.7835] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 49 | Train: 15.4413 | Val: 15.2572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7393/7393 [43:14<00:00,  2.85it/s, Loss=11.4392, Content=6.2939, Style=0.5145] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 50 | Train: 15.5040 | Val: 15.1806\n",
      "üéØ Training Completed!\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms.functional as F\n",
    "loader = get_dataloaders(content_folder=\"../data/coco2017\",\n",
    "                         style_folder=\"../data/wikiart_sampled\", num_workers=64, batch_size=16, persistent_workers=False)\n",
    "path_vgg_weights = \"../models/vgg19.pth\"\n",
    "device = (\"cuda:7\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = AdaINLossMultiLayer(encoder=VGGEncoderMultiLayer(path_vgg_weights=path_vgg_weights, device=device), alpha=1.0, beta=10)\n",
    "model = AdaINet(out_channels=3, path_vgg_weights=path_vgg_weights, device=device).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4*0.5)\n",
    "best_val_loss = train_model(\n",
    "    train_loader=loader[\"train\"],\n",
    "    test_loader=loader[\"valid\"],\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=50,\n",
    "    save_dir=\"checkpoints_adaln\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece9813d",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07449a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os, glob, random\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def load_image(path, device):\n",
    "    \"\"\"Load 1 ·∫£nh, resize v√† chuy·ªÉn th√†nh tensor [1,3,H,W]\"\"\"\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "    ])\n",
    "    img = transform(img).unsqueeze(0).to(device)\n",
    "    return img\n",
    "\n",
    "def unnormalize(tensor):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406], device=tensor.device).view(1, 3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225], device=tensor.device).view(1, 3, 1, 1)\n",
    "    return tensor * std + mean\n",
    "\n",
    "def test_model(model, content_dir, style_dir, device, checkpoint_path, save_dir=\"./test_results\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    content_paths = sorted(glob.glob(os.path.join(content_dir, \"*.jpg\")))\n",
    "    style_paths = sorted(glob.glob(os.path.join(style_dir, \"*.jpg\")))\n",
    "    content_paths = content_paths[:20]\n",
    "\n",
    "    # Load checkpoint\n",
    "    print(f\"üîÑ Loading model weights from {checkpoint_path}\")\n",
    "    ckpt = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(ckpt)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for ci, content_path in enumerate(tqdm(content_paths, desc=\"Testing content\")):\n",
    "            # üëâ ch·ªçn ng·∫´u nhi√™n 1 style cho m·ªói content\n",
    "            style_path = random.choice(style_paths)\n",
    "\n",
    "            # Load ·∫£nh\n",
    "            content = load_image(content_path, device)\n",
    "            style = load_image(style_path, device)\n",
    "\n",
    "            # Save k·∫øt qu·∫£\n",
    "            pair_dir = os.path.join(save_dir, f\"pair_{ci + 1}\")\n",
    "            os.makedirs(pair_dir, exist_ok=True)\n",
    "\n",
    "            save_image(unnormalize(content).clamp(0, 1),\n",
    "                       os.path.join(pair_dir, \"content.jpg\"))\n",
    "            save_image(unnormalize(style).clamp(0, 1),\n",
    "                       os.path.join(pair_dir, \"style.jpg\"))\n",
    "            for alpha in [0.8, 1.0, 1.2]:\n",
    "                generated, _ = model(content, style, alpha=alpha)\n",
    "                \n",
    "                _, _, h, w = content.shape\n",
    "                # resized = F.interpolate(generated, size=(h, w), mode='bilinear', align_corners=False)\n",
    "                out = unnormalize(generated)\n",
    "                # Save\n",
    "                out_path = os.path.join(pair_dir, f\"result_alpha_{alpha:.1f}.jpg\")\n",
    "                save_image(out.clamp(0,1), out_path)\n",
    "\n",
    "    print(f\"‚úÖ Testing completed! Results saved in {save_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e21627bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading model weights from ./checkpoints_adaln/best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing content: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:07<00:00,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Testing completed! Results saved in ../results/adain\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "path_vgg_weights = \"../models/vgg19.pth\"\n",
    "device = (\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AdaINet(out_channels=3, path_vgg_weights=path_vgg_weights, device=device).to(device)\n",
    "test_model(\n",
    "    model=model,\n",
    "    content_dir=\"../data/coco2017/test\",\n",
    "    style_dir=\"../data/wikiart_sampled/test\",\n",
    "    device=device,\n",
    "    checkpoint_path=\"./checkpoints_adaln/best_model.pth\",\n",
    "    save_dir=\"../results/adain\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
